{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import mahotas\n",
    "import cv2\n",
    "import os\n",
    "import h5py\n",
    "import h5py\n",
    "import glob\n",
    "import warnings\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D,Dense,MaxPool2D,Activation,Dropout,Flatten\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import load_model\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "#parameters\n",
    "images_per_class       = 800\n",
    "fixed_size             = tuple((500, 500))\n",
    "train_path             = \"C:/Users/johan/OneDrive/Documents/GitHub/DGMD-S-17/Final_Datasets/Diseased_+_Healthy\"\n",
    "h5_train_data          = 'C:/Users/johan/OneDrive/Documents/GitHub/DGMD-S-17/tier1/output/train_data.h5'\n",
    "h5_train_labels        = 'C:/Users/johan/OneDrive/Documents/GitHub/DGMD-S-17/tier1/output/train_labels.h5'\n",
    "test_dir=\"C:/Users/johan/OneDrive/Documents/GitHub/DGMD-S-17/Final_Datasets/Diseased_+_Healthy\"\n",
    "bins                   = 8\n",
    "Observe_Image = 'PATH'\n",
    "\n",
    "resultsdic = {}\n",
    "finalres = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image format conversions\n",
    "def rgb_bgr(image):\n",
    "    rgb_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return rgb_img\n",
    "def bgr_hsv(rgb_img):\n",
    "    hsv_img = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2HSV)\n",
    "    return hsv_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Manipulation\n",
    "\n",
    "def img_segmentation(rgb_img,hsv_img):\n",
    "    lower_green = np.array([25,0,20])\n",
    "    upper_green = np.array([100,255,255])\n",
    "    healthy_mask = cv2.inRange(hsv_img, lower_green, upper_green)\n",
    "    result = cv2.bitwise_and(rgb_img,rgb_img, mask=healthy_mask)\n",
    "    lower_brown = np.array([10,0,10])\n",
    "    upper_brown = np.array([30,255,255])\n",
    "    disease_mask = cv2.inRange(hsv_img, lower_brown, upper_brown)\n",
    "    disease_result = cv2.bitwise_and(rgb_img, rgb_img, mask=disease_mask)\n",
    "    final_mask = healthy_mask + disease_mask\n",
    "    final_result = cv2.bitwise_and(rgb_img, rgb_img, mask=final_mask)\n",
    "    return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature-descriptor-1: Hu Moments\n",
    "def fd_hu_moments(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    feature = cv2.HuMoments(cv2.moments(image)).flatten()\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature-descriptor-2: Haralick Texture\n",
    "def fd_haralick(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    haralick = mahotas.features.haralick(gray).mean(axis=0)\n",
    "    return haralick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature-descriptor-3: Color Histogram\n",
    "def fd_histogram(image, mask=None):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist  = cv2.calcHist([image], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['diseased', 'healthy']\n"
     ]
    }
   ],
   "source": [
    "# get the training labels\n",
    "train_labels = os.listdir(train_path)\n",
    "\n",
    "# sort the training labels\n",
    "train_labels.sort()\n",
    "print(train_labels)\n",
    "\n",
    "# empty lists to hold feature vectors and labels\n",
    "global_features = []\n",
    "global_features1 = []\n",
    "labels          = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/johan/OneDrive/Documents/GitHub/DGMD-S-17/Final_Datasets/Diseased_+_Healthy/diseased\n",
      "processed folder: diseased\n",
      "C:/Users/johan/OneDrive/Documents/GitHub/DGMD-S-17/Final_Datasets/Diseased_+_Healthy/healthy\n",
      "processed folder: healthy\n",
      "Global Feature Extraction DONE\n"
     ]
    }
   ],
   "source": [
    "# loop over the training data sub-folders\n",
    "for training_name in train_labels:\n",
    "    # join the training data path and each species training folder\n",
    "    dir = train_path + \"/\" + training_name\n",
    "    #dir = os.path.join(train_path, training_name)\n",
    "    print(dir)\n",
    "\n",
    "    # get the current training label\n",
    "    current_label = training_name\n",
    "\n",
    "    # loop over the images in each sub-folder\n",
    "    #for x in range(1,images_per_class+1):\n",
    "    for x in os.listdir(dir):\n",
    "        # get the image file name\n",
    "        file = dir + \"/\" + str(x)\n",
    "\n",
    "        # read the image and resize it to a fixed-size\n",
    "        image = cv2.imread(file)\n",
    "        image = cv2.resize(image, fixed_size)\n",
    "\n",
    "        \n",
    "        # Running Function Bit By Bit\n",
    "        \n",
    "        RGB_BGR       = rgb_bgr(image)\n",
    "        BGR_HSV       = bgr_hsv(RGB_BGR)\n",
    "        IMG_SEGMENT   = img_segmentation(RGB_BGR,BGR_HSV)\n",
    "\n",
    "        # Call for Global Fetaure Descriptors\n",
    "        \n",
    "        fv_hu_moments = fd_hu_moments(IMG_SEGMENT)\n",
    "        fv_haralick   = fd_haralick(IMG_SEGMENT)\n",
    "        fv_histogram  = fd_histogram(IMG_SEGMENT)\n",
    "        \n",
    "        # Concatenate \n",
    "        \n",
    "        global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])\n",
    "        \n",
    "        \n",
    "\n",
    "        # update the list of labels and feature vectors\n",
    "        labels.append(current_label)\n",
    "        global_features.append(global_feature)\n",
    "\n",
    "    print(\"processed folder: {}\".format(current_label))\n",
    "\n",
    "print(\"Global Feature Extraction DONE\")\n",
    "\n",
    "# get the image file name\n",
    "file1 = test_dir + \"/\" + str(Observe_Image)\n",
    "\n",
    "# read the image and resize it to a fixed-size\n",
    "image1 = cv2.imread(file1)\n",
    "image1 = cv2.resize(image1, fixed_size)\n",
    "\n",
    "        \n",
    "# Running Function Bit By Bit\n",
    "        \n",
    "RGB_BGR1       = rgb_bgr(image1)\n",
    "BGR_HSV1       = bgr_hsv(RGB_BGR1)\n",
    "IMG_SEGMENT1   = img_segmentation(RGB_BGR1,BGR_HSV1)\n",
    "\n",
    "# Call for Global Fetaure Descriptors\n",
    "        \n",
    "fv_hu_moments1 = fd_hu_moments(IMG_SEGMENT1)\n",
    "fv_haralick1   = fd_haralick(IMG_SEGMENT1)\n",
    "fv_histogram1  = fd_histogram(IMG_SEGMENT1)\n",
    "        \n",
    "# Concatenate \n",
    "        \n",
    "global_feature1 = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])\n",
    "        \n",
    "        \n",
    "\n",
    "# update the list of labels and feature vectors\n",
    "global_features1.append(global_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] feature vector size (1709, 532)\n"
     ]
    }
   ],
   "source": [
    "# get the overall feature vector size\n",
    "print(\"[STATUS] feature vector size {}\".format(np.array(global_features).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] training Labels (1709,)\n"
     ]
    }
   ],
   "source": [
    "# get the overall training label size\n",
    "print(\"[STATUS] training Labels {}\".format(np.array(labels).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] training labels encoded...\n"
     ]
    }
   ],
   "source": [
    "# encode the target labels\n",
    "targetNames = np.unique(labels)\n",
    "le          = LabelEncoder()\n",
    "target      = le.fit_transform(labels)\n",
    "print(\"[STATUS] training labels encoded...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] feature vector normalized...\n"
     ]
    }
   ],
   "source": [
    "# scale features in the range (0-1)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler            = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaled_features = scaler.fit_transform(global_features)\n",
    "rescaled_features1 = scaler.fit_transform(global_features1)\n",
    "print(\"[STATUS] feature vector normalized...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] target labels: [0 0 0 ... 1 1 1]\n",
      "[STATUS] target labels shape: (1709,)\n"
     ]
    }
   ],
   "source": [
    "print(\"[STATUS] target labels: {}\".format(target))\n",
    "print(\"[STATUS] target labels shape: {}\".format(target.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6tmATL4_ecG"
   },
   "source": [
    "#### **Save Feature Vector using HDF5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/johan/OneDrive/Documents/GitHub/DGMD-S-17/tier1/output/train_data.h5\n"
     ]
    }
   ],
   "source": [
    "print(h5_train_data)\n",
    "h5f_data = h5py.File(h5_train_data, 'w')\n",
    "h5f_data.create_dataset('dataset_1', data=np.array(rescaled_features))\n",
    "\n",
    "h5f_label = h5py.File(h5_train_labels, 'w')\n",
    "h5f_label.create_dataset('dataset_1', data=np.array(target))\n",
    "\n",
    "h5f_data.close()\n",
    "h5f_label.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6tmATL4_ecG"
   },
   "source": [
    "#### **Algorithm Training**\n",
    "**LR, KNN, SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] features shape: (1709, 532)\n",
      "[STATUS] labels shape: (1709,)\n",
      "[STATUS] training started...\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "#-----------------------------------\n",
    "# TRAINING OUR MODEL\n",
    "#-----------------------------------\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#--------------------\n",
    "# tunable-parameters\n",
    "#--------------------\n",
    "num_trees = 100\n",
    "test_size = 0.20\n",
    "seed      = 9\n",
    "train_path = \"C:/Users/johan/OneDrive/Documents/GitHub/DGMD-S-17/Final_Datasets/Diseased_+_Healthy\"\n",
    "test_path  = \"C:/Users/johan/OneDrive/Documents/GitHub/DGMD-S-17/tier1/test\"\n",
    "h5_train_data    = 'C:/Users/johan/OneDrive/Documents/GitHub/DGMD-S-17/tier1/output/train_data.h5'\n",
    "h5_train_labels  = 'C:/Users/johan/OneDrive/Documents/GitHub/DGMD-S-17/tier1/output/train_labels.h5'\n",
    "scoring    = \"accuracy\"\n",
    "\n",
    "# get the training labels\n",
    "train_labels = os.listdir(train_path)\n",
    "\n",
    "# sort the training labels\n",
    "train_labels.sort()\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    os.makedirs(test_path)\n",
    "\n",
    "# create all the machine learning models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(random_state=seed)))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('SVM', SVC(random_state=seed)))\n",
    "\n",
    "\n",
    "# variables to hold the results and names\n",
    "results = []\n",
    "names   = []\n",
    "\n",
    "# import the feature vector and trained labels\n",
    "h5f_data  = h5py.File(h5_train_data, 'r')\n",
    "h5f_label = h5py.File(h5_train_labels, 'r')\n",
    "\n",
    "global_features_string = h5f_data['dataset_1']\n",
    "global_labels_string   = h5f_label['dataset_1']\n",
    "\n",
    "global_features = np.array(global_features_string)\n",
    "global_labels   = np.array(global_labels_string)\n",
    "\n",
    "h5f_data.close()\n",
    "h5f_label.close()\n",
    "\n",
    "# verify the shape of the feature vector and labels\n",
    "print(\"[STATUS] features shape: {}\".format(global_features.shape))\n",
    "print(\"[STATUS] labels shape: {}\".format(global_labels.shape))\n",
    "print(\"[STATUS] training started...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] split train and test data...\n",
      "Train data  : (1367, 532)\n",
      "Test data   : (342, 532)\n"
     ]
    }
   ],
   "source": [
    "# split the training and testing data\n",
    "(trainDataGlobal, testDataGlobal, trainLabelsGlobal, testLabelsGlobal) = train_test_split(np.array(global_features),\n",
    "                                                                                          np.array(global_labels),\n",
    "                                                                                          test_size=test_size,\n",
    "                                                                                          random_state=seed)\n",
    "\n",
    "print(\"[STATUS] split train and test data...\")\n",
    "print(\"Train data  : {}\".format(trainDataGlobal.shape))\n",
    "print(\"Test data   : {}\".format(testDataGlobal.shape)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1367, 532)\n"
     ]
    }
   ],
   "source": [
    "trainDataGlobal\n",
    "print(trainDataGlobal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.917341 (0.043566)\n",
      "KNN: 0.930523 (0.025313)\n",
      "SVM: 0.918076 (0.038037)\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross validation, iterating over the three algorithms.\n",
    "\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = cross_val_score(model, trainDataGlobal, trainLabelsGlobal, cv=kfold, scoring=scoring)\n",
    "    names.append(name)\n",
    "    resultsdic[name] = cv_results.mean()\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN has highest score. Using KNeighborsClassifier()\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "#clf.fit(trainDataGlobal, trainLabelsGlobal)\n",
    "\n",
    "Keymax = max(zip(resultsdic.values(), resultsdic.keys()))[1]\n",
    "ik = 0\n",
    "while ik < len(models) - 1:\n",
    "    if Keymax == models[ik][0]:\n",
    "        m = models[ik][1]\n",
    "        print(Keymax + ' has highest score. Using ' + str(m))\n",
    "    else:\n",
    "        pass\n",
    "    ik += 1\n",
    "\n",
    "m.fit(trainDataGlobal, trainLabelsGlobal)\n",
    "y_predict = m.predict(rescaled_features1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92       157\n",
      "           1       0.91      0.96      0.93       185\n",
      "\n",
      "    accuracy                           0.93       342\n",
      "   macro avg       0.93      0.92      0.93       342\n",
      "weighted avg       0.93      0.93      0.93       342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(testLabelsGlobal,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy score for Tier 1: 0.9269005847953217\n"
     ]
    }
   ],
   "source": [
    "tier1_accuracy_score = accuracy_score(testLabelsGlobal, y_predict)\n",
    "print('Overall Accuracy score for Tier 1: ' + str(tier1_accuracy_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6tmATL4_ecG"
   },
   "source": [
    "#### **Test-Train Data**\n",
    "**Split the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "907a8041"
   },
   "outputs": [],
   "source": [
    "def get_files(directory):\n",
    "  if not os.path.exists(directory):\n",
    "    return 0\n",
    "  count=0\n",
    "  for current_path,dirs,files in os.walk(directory):\n",
    "    for dr in dirs:\n",
    "      count+= len(glob.glob(os.path.join(current_path,dr+\"/*\")))\n",
    "  return count\n",
    "train_dir =\"C:/Users/johan/OneDrive/Documents/GitHub/DGMD-S-17/Final_Datasets/CNN_Dataset\"\n",
    "test_dir = \"C:/Users/johan/OneDrive/Documents/GitHub/DGMD-S-17/tier2/test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a07bcaa8",
    "outputId": "66fa1298-e35a-466b-e130-57aba5081e4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 Classes\n",
      "45225 Train images\n",
      "0 Test images\n"
     ]
    }
   ],
   "source": [
    "#image count\n",
    "train_samples =get_files(train_dir)\n",
    "num_classes=len(glob.glob(train_dir+\"/*\"))\n",
    "test_samples=get_files(test_dir)\n",
    "print(num_classes,\"Classes\")\n",
    "print(train_samples,\"Train images\")\n",
    "print(test_samples,\"Test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuGK0vnQ_jRe"
   },
   "source": [
    "#### **Image Augumentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "d3d73be1"
   },
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    "    )\n",
    "test_datagen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6WpHBoue9d3M",
    "outputId": "374ea870-631c-4fd8-d6a5-17d03d8d7b90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45225 images belonging to 38 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "input_shape=(224,224,3)\n",
    "train_generator =train_datagen.flow_from_directory(train_dir,target_size=(224,224),batch_size=32)\n",
    "test_generator=test_datagen.flow_from_directory(test_dir,shuffle=True,target_size=(224,224),batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHhlQnxs_SXv"
   },
   "source": [
    "#### **CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c7b4d1a",
    "outputId": "5f6d1d65-0079-44c9-8dd2-070697dff741"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_1 (Conv2D)           (None, 220, 220, 32)      2432      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 73, 73, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 71, 71, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 35, 35, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 33, 33, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 16384)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               8389120   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 38)                4902      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,489,862\n",
      "Trainable params: 8,489,862\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5),input_shape=input_shape,activation='relu',name=\"conv2d_1\"))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3),name=\"max_pooling2d_1\"))\n",
    "model.add(Conv2D(32, (3, 3),activation='relu',name=\"conv2d_2\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name=\"max_pooling2d_2\"))\n",
    "model.add(Conv2D(64, (3, 3),activation='relu',name=\"conv2d_3\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),name=\"max_pooling2d_3\"))   \n",
    "model.add(Flatten(name=\"flatten_1\"))\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128,activation='relu'))          \n",
    "model.add(Dense(num_classes,activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6daeef2d",
    "outputId": "87c475e4-1c41-45dd-9c9d-f438f3b5cfae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = train_datagen.flow_from_directory(\n",
    "                       test_dir,\n",
    "                       target_size=(224, 224),\n",
    "                       batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d3b4edf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "946/946 [==============================] - ETA: 0s - loss: 1.1014 - accuracy: 0.6635WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "946/946 [==============================] - 544s 574ms/step - loss: 1.1014 - accuracy: 0.6635 - lr: 0.0010\n",
      "Epoch 2/2\n",
      "946/946 [==============================] - ETA: 0s - loss: 0.5037 - accuracy: 0.8378WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
      "946/946 [==============================] - 560s 591ms/step - loss: 0.5037 - accuracy: 0.8378 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss = 'categorical_crossentropy',metrics=['accuracy'])\n",
    "history1 = model.fit(\n",
    "    train_generator,#egitim verileri\n",
    "    steps_per_epoch=None,\n",
    "    epochs=2,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=None,\n",
    "    verbose=1,\n",
    "    callbacks=[ReduceLROnPlateau(monitor='val_loss', factor=0.3,patience=3, min_lr=0.000001)],\n",
    "    shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a616bb60"
   },
   "outputs": [],
   "source": [
    "model.save('/tier2/plant_disease_Cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YybBG0FDCCMQ"
   },
   "source": [
    "#### **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "bc186b0d",
    "outputId": "480c8413-d16a-446f-a3c7-de419d0063dd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-142-d839fe0915b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_cnn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/tier2/plant_disease_Cnn.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model_cnn=load_model('/tier2/plant_disease_Cnn.h5')\n",
    "\n",
    "def prepare(img_path):\n",
    "    img = tf.keras.utils.load_img(img_path, target_size=(224,224))\n",
    "    x = tf.keras.utils.img_to_array(img)\n",
    "    x = x/255\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "#Looping over images in testing.\n",
    "for test_file in os.listdir(test_dir):\n",
    "    #print(test_file)\n",
    "    classes=list(train_generator.class_indices.keys())\n",
    "    img_url= test_dir + '/' + test_file\n",
    "    result_cnn = model_cnn.predict([prepare(img_url)]) #Passes through the model after preparing the image.\n",
    "    disease = mpimg.imread(img_url)\n",
    "    plt.figure()\n",
    "    #plt.imshow(disease) #present images as figure\n",
    "\n",
    "    classresult=np.argmax(result_cnn,axis=1)\n",
    "    #print(classes[classresult[0]])\n",
    "\n",
    "print(Observe_Image)\n",
    "classes=list(train_generator.class_indices.keys())\n",
    "result_cnn = model_cnn.predict([prepare(Observe_Image)]) \n",
    "disease = mpimg.imread(Observe_Image)\n",
    "plt.figure()\n",
    "plt.imshow(disease)\n",
    "classresult=np.argmax(result_cnn,axis=1)\n",
    "print(classes[classresult[0]])\n",
    "if y_predict[0] == 1:\n",
    "    if classes[classresult[0]].find('healthy') == -1:\n",
    "        print('True_Positive')\n",
    "    else:\n",
    "        print('False_Negative')\n",
    "else:\n",
    "    if classes[classresult[0]].find('healthy') == -1:\n",
    "        print('False_Positive')\n",
    "    else:\n",
    "        print('True_Negative')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "04c2774e0eee72fdf7c569530da90ed42b07f746012a791613028ba7115aa244"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
